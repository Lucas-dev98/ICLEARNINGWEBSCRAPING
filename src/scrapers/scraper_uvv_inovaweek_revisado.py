#!/usr/bin/env python3
"""
Scraper Espec√≠fico para Not√≠cias UVV - InovaWeek (Agosto-Setembro 2025)
=======================================================================

Este script coleta especificamente as not√≠cias da Universidade Vila Velha (UVV)
relacionadas ao InovaWeek entre agosto e setembro de 2025.

Funcionalidades:
- Coleta de not√≠cias do site oficial da UVV sobre InovaWeek
- Filtragem por per√≠odo (agosto-setembro 2025)
- Extra√ß√£o de dados espec√≠ficos: t√≠tulo, link, conte√∫do, autor, data de publica√ß√£o
- Tratamento robusto de datas com biblioteca datetime
- Requisi√ß√µes HTTP otimizadas com biblioteca requests
- Verifica√ß√µes de argumentos das tags CSS
- Exporta√ß√£o para CSV estruturado

Autor: ICLearning WebScraping Project
Data: 2025-09-25
URL Alvo: https://www.uvv.br/noticias/
Per√≠odo: Agosto-Setembro 2025
Foco: Not√≠cias do InovaWeek
"""

# === IMPORTA√á√ïES NECESS√ÅRIAS ===
import requests
import csv
from bs4 import BeautifulSoup
import time
import re
import json
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse
import os
import argparse
import sys
from collections import defaultdict

class UVVInovaWeekScraper:
    """
    Scraper especializado para not√≠cias do InovaWeek da UVV.
    
    Coleta not√≠cias espec√≠ficas do InovaWeek entre agosto e setembro de 2025,
    com verifica√ß√µes robustas de tags CSS e extra√ß√£o precisa de dados.
    """
    
    def __init__(self, periodo_inicio=None, periodo_fim=None, seletores_customizados=None):
        """
        Inicializa o scraper do InovaWeek UVV.
        
        Args:
            periodo_inicio (datetime): Data de in√≠cio da coleta (padr√£o: agosto 2025)
            periodo_fim (datetime): Data de fim da coleta (padr√£o: setembro 2025)
            seletores_customizados (dict): Seletores CSS customizados
        """
        # Configura√ß√µes de per√≠odo
        self.periodo_inicio = periodo_inicio or datetime(2025, 8, 1)
        self.periodo_fim = periodo_fim or datetime(2025, 9, 30, 23, 59, 59)
        
        # URLs de destino
        self.base_url = "https://www.uvv.br"
        self.noticias_url = "https://www.uvv.br/noticias/"
        
        # Headers para requisi√ß√µes HTTP
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0'
        }
        
        # Session configurada seguindo melhores pr√°ticas requests
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # Configura√ß√µes avan√ßadas da session
        # Reutilizar conex√µes TCP (pool de conex√µes)
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        # Estrat√©gia de retry com backoff
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            backoff_factor=1,
            raise_on_status=False
        )
        
        # Adapter com pool de conex√µes
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=20
        )
        
        # Montar adapters para HTTP e HTTPS
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Configurar cookies jar para persistir cookies
        # N√£o definir policy como None - usar a padr√£o do requests
        
        # Seletores CSS para extra√ß√£o de dados
        self.seletores = self._configurar_seletores(seletores_customizados)
        
        # Armazenamento das not√≠cias coletadas
        self.noticias = []
        
        # Timestamp do in√≠cio do scraping
        self.timestamp_scraping = datetime.now()
        
        # Palavras-chave para identificar not√≠cias do InovaWeek
        self.palavras_chave_inova = [
            'inovaweek', 'inova week', 'inova', 'inova√ß√£o', 'innovation',
            'semana de inova√ß√£o', 'evento de inova√ß√£o', 'empreendedorismo',
            'startup', 'hackathon', 'pitch', 'palestra inova√ß√£o'
        ]
        
        print(f"üöÄ UVV InovaWeek Scraper inicializado")
        print(f"üìÖ Per√≠odo de coleta: {self.periodo_inicio.strftime('%d/%m/%Y')} at√© {self.periodo_fim.strftime('%d/%m/%Y')}")
        print(f"üïê Scraping iniciado em: {self.timestamp_scraping.strftime('%d/%m/%Y %H:%M:%S')}")
    
    def _configurar_seletores(self, seletores_customizados=None):
        """
        Configura os seletores CSS para extra√ß√£o de dados.
        
        Args:
            seletores_customizados (dict): Seletores customizados do usu√°rio
            
        Returns:
            dict: Dicion√°rio com todos os seletores configurados
        """
        seletores_padrao = {
            # Seletores para identificar not√≠cias na p√°gina principal
            'container_noticias': [
                '.col-md-3',
                '.card',
                '.news-item',
                '.post-item',
                'article',
                '.noticia-item',
                'div[class*="col-"]'
            ],
            
            # Seletores para t√≠tulo da not√≠cia
            'titulo': [
                'h1',
                'h2', 
                'h3',
                '.titulo',
                '.title',
                '.headline',
                '.news-title',
                '.post-title',
                'a[title]'
            ],
            
            # Seletores para links
            'link': [
                'a[href]'
            ],
            
            # Seletores para autor
            'autor': [
                '.autor',
                '.author',
                '.by',
                '.byline',
                '.post-author',
                '.news-author',
                '[class*="author"]'
            ],
            
            # Seletores para data de publica√ß√£o
            'data_publicacao': [
                '.data',
                '.date',
                '.published',
                '.post-date',
                '.news-date',
                'time',
                '[datetime]',
                '[class*="date"]'
            ],
            
            # Seletores para conte√∫do completo (p√°gina individual)
            'conteudo_completo': [
                '.single-post .entry-content',
                '.post-content .entry-content',
                '.news-content',
                '.noticia-texto',
                '.article-body',
                '.post-body',
                '.entry-text',
                '.content-wrapper',
                '.post-content',
                '.entry-content',
                '.article-content',
                '.content-area',
                '.single-post-content',
                '.main-content',
                'article .content',
                'main article',
                '.texto-noticia'
            ]
        }
        
        # Mesclar com seletores customizados se fornecidos
        if seletores_customizados:
            for categoria, seletores in seletores_customizados.items():
                if categoria in seletores_padrao:
                    seletores_padrao[categoria].extend(seletores)
                else:
                    seletores_padrao[categoria] = seletores
        
        return seletores_padrao
    
    def fazer_requisicao(self, url, params=None, timeout=30, max_tentativas=3):
        """
        Faz requisi√ß√£o HTTP com tratamento robusto de erros seguindo melhores pr√°ticas.
        Implementa uso correto de par√¢metros URL conforme documenta√ß√£o requests.
        
        Args:
            url (str): URL base para requisi√ß√£o
            params (dict): Par√¢metros URL a serem codificados automaticamente
            timeout (int): Timeout em segundos
            max_tentativas (int): N√∫mero m√°ximo de tentativas
            
        Returns:
            requests.Response or None: Resposta ou None se erro
        """
        for tentativa in range(max_tentativas):
            try:
                # Log da URL com par√¢metros (se houver)
                if params:
                    print(f"üîç Fazendo requisi√ß√£o (tentativa {tentativa + 1}): {url} com params: {params}")
                else:
                    print(f"üîç Fazendo requisi√ß√£o (tentativa {tentativa + 1}): {url}")
                
                # Requisi√ß√£o usando melhores pr√°ticas do requests
                response = self.session.get(
                    url, 
                    params=params,  # Par√¢metros URL codificados automaticamente
                    timeout=timeout,
                    allow_redirects=True  # Permitir redirecionamentos
                )
                
                # Verificar status HTTP usando raise_for_status()
                response.raise_for_status()
                
                # Configurar encoding corretamente se necess√°rio
                if response.encoding is None or response.encoding == 'ISO-8859-1':
                    response.encoding = 'utf-8'
                
                print(f"‚úÖ Status: {response.status_code} | URL Final: {response.url}")
                print(f"üìä Tamanho: {len(response.content)} bytes | Encoding: {response.encoding}")
                return response
                
            except requests.exceptions.Timeout as e:
                print(f"‚è∞ Timeout na tentativa {tentativa + 1} para {url}: {e}")
                if tentativa < max_tentativas - 1:
                    time.sleep(2 ** tentativa)  # Backoff exponencial
            except requests.exceptions.ConnectionError as e:
                print(f"üîå Erro de conex√£o na tentativa {tentativa + 1} para {url}: {e}")
                if tentativa < max_tentativas - 1:
                    time.sleep(2 ** tentativa)
            except requests.exceptions.HTTPError as e:
                print(f"üö´ Erro HTTP na tentativa {tentativa + 1} para {url}: {e}")
                if tentativa < max_tentativas - 1:
                    time.sleep(2 ** tentativa)
            except requests.exceptions.RequestException as e:
                print(f"‚ùå Erro geral na tentativa {tentativa + 1} para {url}: {e}")
                if tentativa < max_tentativas - 1:
                    time.sleep(2 ** tentativa)
        
        print(f"‚ùå Falhou ap√≥s {max_tentativas} tentativas")
        return None
    
    def construir_url_busca(self, termo_busca=None, pagina=1, data_inicio=None, data_fim=None):
        """
        Constr√≥i URL de busca com par√¢metros usando melhores pr√°ticas requests.
        
        Args:
            termo_busca (str): Termo para buscar
            pagina (int): N√∫mero da p√°gina
            data_inicio (datetime): Data inicial para filtro
            data_fim (datetime): Data final para filtro
            
        Returns:
            tuple: (url_base, params_dict) para uso com requests
        """
        # URL base para busca
        url_base = self.noticias_url
        
        # Construir par√¢metros usando dict (melhor pr√°tica requests)
        params = {}
        
        if termo_busca:
            params['q'] = termo_busca
            params['search'] = termo_busca
            
        if pagina and pagina > 1:
            params['page'] = pagina
            params['p'] = pagina
            
        if data_inicio:
            params['date_from'] = data_inicio.strftime('%Y-%m-%d')
            
        if data_fim:
            params['date_to'] = data_fim.strftime('%Y-%m-%d')
        
        # Par√¢metros adicionais que podem ser √∫teis
        params['category'] = 'noticias'
        params['type'] = 'news'
        
        print(f"üîß URL constru√≠da: {url_base}")
        if params:
            print(f"üìã Par√¢metros URL: {params}")
            
        return url_base, params
    
    def testar_conectividade(self):
        """
        Testa conectividade com o site usando melhores pr√°ticas requests.
        
        Returns:
            bool: True se conectividade OK, False caso contr√°rio
        """
        try:
            print("üîß Testando conectividade com o site...")
            
            # Fazer uma requisi√ß√£o HEAD primeiro (mais leve)
            response = self.session.head(self.base_url, timeout=10, allow_redirects=True)
            
            if response.status_code in [200, 301, 302]:
                print(f"‚úÖ Conectividade OK - Status: {response.status_code}")
                print(f"üåê URL final: {response.url}")
                print(f"üìÑ Server: {response.headers.get('server', 'N/A')}")
                return True
            else:
                print(f"‚ö†Ô∏è  Status inesperado: {response.status_code}")
                return False
                
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Erro de conectividade: {e}")
            return False
    
    def configurar_proxy(self, proxy_config=None):
        """
        Configura proxy para a session se necess√°rio.
        
        Args:
            proxy_config (dict): Configura√ß√£o de proxy
                                Exemplo: {'http': 'http://proxy:port', 'https': 'https://proxy:port'}
        """
        if proxy_config:
            self.session.proxies.update(proxy_config)
            print(f"üîß Proxy configurado: {proxy_config}")
    
    def extrair_data_noticia(self, elemento_html, texto_elemento=None):
        """
        Extrai e converte data da not√≠cia com verifica√ß√µes robustas.
        
        Args:
            elemento_html: Elemento HTML contendo a data
            texto_elemento (str): Texto alternativo para buscar data
            
        Returns:
            tuple: (datetime object, string original) ou (None, None)
        """
        # Textos onde procurar data
        textos_para_buscar = []
        
        if elemento_html:
            # Verificar atributos datetime primeiro
            for attr in ['datetime', 'data-date', 'data-time']:
                if elemento_html.get(attr):
                    textos_para_buscar.append(elemento_html.get(attr))
            
            # Adicionar texto do elemento
            texto_elem = elemento_html.get_text().strip()
            if texto_elem:
                textos_para_buscar.append(texto_elem)
        
        if texto_elemento:
            textos_para_buscar.append(texto_elemento)
        
        # Padr√µes de data em portugu√™s
        padroes_data = [
            (r'(\d{1,2})[\/\-](\d{1,2})[\/\-](\d{4})', 'dd/mm/yyyy'),
            (r'(\d{4})[\/\-](\d{1,2})[\/\-](\d{1,2})', 'yyyy/mm/dd'),
            (r'(\d{1,2}) de (\w+) de (\d{4})', 'dd de m√™s de yyyy'),
            (r'(\d{1,2}) (\w+) (\d{4})', 'dd m√™s yyyy'),
            (r'(\w+) (\d{1,2}), (\d{4})', 'm√™s dd, yyyy'),
            (r'(\d{1,2})[\/\-](\d{1,2})[\/\-](\d{2})', 'dd/mm/yy')
        ]
        
        # Mapeamento de meses em portugu√™s
        meses_pt = {
            'janeiro': 1, 'jan': 1,
            'fevereiro': 2, 'fev': 2,
            'mar√ßo': 3, 'mar': 3,
            'abril': 4, 'abr': 4,
            'maio': 5, 'mai': 5,
            'junho': 6, 'jun': 6,
            'julho': 7, 'jul': 7,
            'agosto': 8, 'ago': 8,
            'setembro': 9, 'set': 9,
            'outubro': 10, 'out': 10,
            'novembro': 11, 'nov': 11,
            'dezembro': 12, 'dez': 12
        }
        
        for texto in textos_para_buscar:
            texto = texto.strip().lower()
            
            for padrao, formato in padroes_data:
                match = re.search(padrao, texto)
                if match:
                    try:
                        grupos = match.groups()
                        
                        if formato == 'dd/mm/yyyy':
                            dia, mes, ano = map(int, grupos)
                        elif formato == 'yyyy/mm/dd':
                            ano, mes, dia = map(int, grupos)
                        elif formato in ['dd de m√™s de yyyy', 'dd m√™s yyyy']:
                            dia, nome_mes, ano = grupos
                            dia, ano = int(dia), int(ano)
                            mes = meses_pt.get(nome_mes.lower())
                            if not mes:
                                continue
                        elif formato == 'm√™s dd, yyyy':
                            nome_mes, dia, ano = grupos
                            dia, ano = int(dia), int(ano)
                            mes = meses_pt.get(nome_mes.lower())
                            if not mes:
                                continue
                        elif formato == 'dd/mm/yy':
                            dia, mes, ano = map(int, grupos)
                            ano = 2000 + ano if ano < 50 else 1900 + ano
                        
                        # Validar data
                        data_obj = datetime(ano, mes, dia)
                        return data_obj, match.group(0)
                        
                    except (ValueError, TypeError):
                        continue
        
        return None, None
    
    def eh_noticia_inovaweek(self, titulo, conteudo_texto="", periodo_valido=True):
        """
        Verifica se a not√≠cia √© relacionada ao InovaWeek.
        
        Args:
            titulo (str): T√≠tulo da not√≠cia
            conteudo_texto (str): Conte√∫do da not√≠cia
            periodo_valido (bool): Se a data est√° no per√≠odo v√°lido
            
        Returns:
            bool: True se for not√≠cia do InovaWeek
        """
        # Texto completo para busca
        texto_completo = f"{titulo} {conteudo_texto}".lower()
        
        # Verificar palavras-chave do InovaWeek
        for palavra in self.palavras_chave_inova:
            if palavra.lower() in texto_completo:
                return True
        
        return False
    
    def eh_periodo_valido(self, data_noticia):
        """
        Verifica se a data da not√≠cia est√° no per√≠odo v√°lido (agosto-setembro 2025).
        
        Args:
            data_noticia (datetime): Data da not√≠cia
            
        Returns:
            bool: True se estiver no per√≠odo v√°lido
        """
        if not data_noticia:
            return False
        
        return self.periodo_inicio <= data_noticia <= self.periodo_fim
    
    def extrair_com_seletores(self, soup, categoria_seletor, elemento_pai=None):
        """
        Extrai elemento usando m√∫ltiplos seletores CSS com verifica√ß√µes.
        
        Args:
            soup: Objeto BeautifulSoup
            categoria_seletor (str): Categoria de seletor ('titulo', 'autor', etc.)
            elemento_pai: Elemento pai para busca espec√≠fica
            
        Returns:
            BeautifulSoup element or None: Primeiro elemento encontrado
        """
        base_soup = elemento_pai if elemento_pai else soup
        seletores = self.seletores.get(categoria_seletor, [])
        
        for seletor in seletores:
            try:
                elementos = base_soup.select(seletor)
                if elementos:
                    # Verificar se o elemento tem conte√∫do v√°lido
                    elemento = elementos[0]
                    texto = elemento.get_text().strip()
                    
                    # Valida√ß√µes espec√≠ficas por categoria
                    if categoria_seletor == 'titulo' and len(texto) < 10:
                        continue
                    elif categoria_seletor == 'autor' and len(texto) < 2:
                        continue
                    elif categoria_seletor == 'link' and not elemento.get('href'):
                        continue
                    
                    return elemento
                        
            except Exception as e:
                print(f"‚ö†Ô∏è Erro no seletor '{seletor}' para {categoria_seletor}: {e}")
                continue
        
        return None
    
    def processar_noticia_individual(self, url_noticia):
        """
        Processa uma not√≠cia individual para extrair conte√∫do completo.
        
        Args:
            url_noticia (str): URL da not√≠cia espec√≠fica
            
        Returns:
            dict: Dados completos da not√≠cia
        """
        # Rate limiting (ser respeitoso com o servidor)
        time.sleep(1)
        
        # Fazer requisi√ß√£o usando melhores pr√°ticas (sem par√¢metros adicionais para p√°gina individual)
        response = self.fazer_requisicao(url_noticia, params=None)
        if not response:
            return {'conteudo_completo': '', 'erro': 'Falha na requisi√ß√£o'}
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Remover elementos desnecess√°rios
        for elemento in soup.select('script, style, nav, header, footer, aside, .menu, .navigation'):
            elemento.decompose()
        
        # Extrair conte√∫do principal
        elemento_conteudo = self.extrair_com_seletores(soup, 'conteudo_completo')
        conteudo_texto = ""
        
        if elemento_conteudo:
            # Extrair par√°grafos do conte√∫do
            paragrafos = elemento_conteudo.find_all(['p', 'div'], string=True)
            textos_limpos = []
            
            for p in paragrafos:
                texto_p = p.get_text().strip()
                # Filtrar textos muito curtos ou de navega√ß√£o
                if (len(texto_p) > 30 and 
                    not self._eh_texto_navegacao(texto_p)):
                    textos_limpos.append(texto_p)
            
            conteudo_texto = '\n\n'.join(textos_limpos)
        
        # Se n√£o encontrou conte√∫do com seletores, tentar estrat√©gia alternativa
        if not conteudo_texto or len(conteudo_texto) < 100:
            paragrafos_alternativos = soup.find_all('p')
            textos_alternativos = []
            
            for p in paragrafos_alternativos:
                texto_p = p.get_text().strip()
                if (len(texto_p) > 50 and 
                    not self._eh_texto_navegacao(texto_p)):
                    textos_alternativos.append(texto_p)
            
            if textos_alternativos:
                conteudo_texto = '\n\n'.join(textos_alternativos)
        
        return {
            'conteudo_completo': conteudo_texto,
            'tamanho_conteudo': len(conteudo_texto)
        }
    
    def _eh_texto_navegacao(self, texto):
        """Verifica se o texto parece ser de navega√ß√£o/menu."""
        texto_lower = texto.lower()
        
        # Palavras t√≠picas de navega√ß√£o da UVV
        palavras_navegacao = [
            'institucional', 'gradua√ß√£o', 'p√≥s gradua√ß√£o', 'mestrado',
            'doutorado', 'pesquisa', 'extens√£o', 'contato', 'blog',
            'not√≠cias', 'eventos', 'portal', 'trabalhe conosco'
        ]
        
        # Se cont√©m principalmente palavras de navega√ß√£o
        palavras_nav_encontradas = sum(1 for palavra in palavras_navegacao if palavra in texto_lower)
        palavras_totais = len(texto.split())
        
        if palavras_totais > 0 and (palavras_nav_encontradas / palavras_totais) > 0.5:
            return True
        
        # Texto muito curto
        if len(texto) < 20:
            return True
            
        return False
    
    def extrair_noticias_pagina(self, html_content, url_pagina):
        """
        Extrai not√≠cias de uma p√°gina espec√≠fica.
        
        Args:
            html_content (str): HTML da p√°gina
            url_pagina (str): URL da p√°gina atual
            
        Returns:
            list: Lista de not√≠cias extra√≠das
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        noticias_encontradas = []
        
        print(f"üìÑ Analisando p√°gina: {url_pagina}")
        
        # Buscar containers de not√≠cias com m√∫ltiplas estrat√©gias
        containers = set()  # Use set para evitar duplicatas
        
        # Estrat√©gia 1: Usar seletores configurados
        for seletor in self.seletores['container_noticias']:
            elementos = soup.select(seletor)
            for elem in elementos:
                containers.add(elem)
        
        # Estrat√©gia 2: Buscar elementos que contenham links para not√≠cias
        links_noticias = soup.find_all('a', href=True)
        for link in links_noticias:
            href = link.get('href', '')
            if '/noticias/' in href and len(link.get_text().strip()) > 20:
                # Adicionar o container pai do link
                container_pai = link.find_parent(['div', 'article', 'section'])
                if container_pai:
                    containers.add(container_pai)
        
        # Estrat√©gia 3: Buscar por elementos com texto que contenha palavras-chave
        for palavra_chave in ['inovaweek', 'inova', 'setembro', 'agosto']:
            elementos_texto = soup.find_all(string=re.compile(palavra_chave, re.IGNORECASE))
            for texto in elementos_texto:
                if hasattr(texto, 'parent'):
                    container_texto = texto.parent.find_parent(['div', 'article', 'section'])
                    if container_texto:
                        containers.add(container_texto)
        
        containers = list(containers)  # Converter de volta para lista
        print(f"üîç Encontrados {len(containers)} containers √∫nicos de not√≠cias")
        
        for i, container in enumerate(containers):
            try:
                noticia_data = self._extrair_dados_noticia(container, url_pagina)
                
                if noticia_data and noticia_data.get('titulo'):
                    print(f"üìã {i+1}/{len(containers)} Analisando: {noticia_data['titulo'][:60]}...")
                    
                    # Verificar se √© not√≠cia do InovaWeek (mais flex√≠vel)
                    eh_inovaweek = self.eh_noticia_inovaweek(
                        noticia_data['titulo'],
                        noticia_data.get('resumo', ''),
                        True  # Mais flex√≠vel com per√≠odo inicialmente
                    )
                    
                    if eh_inovaweek:
                        noticias_encontradas.append(noticia_data)
                        print(f"‚úÖ Not√≠cia InovaWeek encontrada: {noticia_data['titulo'][:60]}...")
                    else:
                        print(f"‚ùå N√£o √© InovaWeek: {noticia_data['titulo'][:60]}...")
                else:
                    print(f"‚ö†Ô∏è {i+1}/{len(containers)} Container sem t√≠tulo v√°lido")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao processar container {i+1}: {e}")
                continue
        
        return noticias_encontradas
    
    def _extrair_dados_noticia(self, container, url_pagina):
        """
        Extrai dados de uma not√≠cia espec√≠fica do container HTML.
        
        Args:
            container: Elemento BeautifulSoup do container
            url_pagina (str): URL da p√°gina atual
            
        Returns:
            dict: Dados da not√≠cia extra√≠dos
        """
        noticia = {
            'titulo': '',
            'link': '',
            'autor': '',
            'data_publicacao': None,
            'data_publicacao_str': '',
            'resumo': '',
            'conteudo_completo': '',
            'timestamp_coleta': self.timestamp_scraping.isoformat(),
            'url_fonte': url_pagina,
            'periodo_valido': False
        }
        
        # Extrair t√≠tulo com m√∫ltiplas estrat√©gias
        elem_titulo = self.extrair_com_seletores(container, 'titulo', container)
        if elem_titulo:
            noticia['titulo'] = elem_titulo.get_text().strip()
        else:
            # Estrat√©gia alternativa: buscar qualquer texto significativo
            textos = container.find_all(string=True)
            for texto in textos:
                texto_limpo = texto.strip()
                if len(texto_limpo) > 20 and not self._eh_texto_navegacao(texto_limpo):
                    noticia['titulo'] = texto_limpo
                    break
        
        # Filtrar t√≠tulos muito curtos (mais flex√≠vel)
        if len(noticia['titulo']) < 5:
            return None
        
        # Extrair link
        elem_link = self.extrair_com_seletores(container, 'link', container)
        if elem_link and elem_link.get('href'):
            link = elem_link.get('href')
            if link.startswith('/'):
                link = urljoin(self.base_url, link)
            elif link.startswith('#'):
                return None  # Skip anchor links
            noticia['link'] = link
        
        # Extrair autor
        elem_autor = self.extrair_com_seletores(container, 'autor', container)
        if elem_autor:
            noticia['autor'] = elem_autor.get_text().strip()
        
        # Extrair data de publica√ß√£o
        elem_data = self.extrair_com_seletores(container, 'data_publicacao', container)
        data_obj, data_str = self.extrair_data_noticia(elem_data, container.get_text())
        
        if data_obj:
            noticia['data_publicacao'] = data_obj
            noticia['data_publicacao_str'] = data_str
            noticia['periodo_valido'] = self.eh_periodo_valido(data_obj)
        
        # Extrair resumo (se dispon√≠vel)
        paragrafos = container.find_all('p')
        if paragrafos:
            for p in paragrafos:
                texto_p = p.get_text().strip()
                if len(texto_p) > 50 and not self._eh_texto_navegacao(texto_p):
                    noticia['resumo'] = texto_p[:300]
                    break
        
        return noticia
    
    def coletar_noticias_inovaweek(self, max_paginas=10, somente_primeira_pagina=False):
        """
        M√©todo principal para coletar not√≠cias do InovaWeek com suporte a pagina√ß√£o.
        
        Args:
            max_paginas (int): N√∫mero m√°ximo de p√°ginas para vasculhar (padr√£o: 10)
            somente_primeira_pagina (bool): Se deve coletar apenas a primeira p√°gina
        
        Returns:
            list: Lista de not√≠cias do InovaWeek coletadas de todas as p√°ginas
        """
        print(f"\nüöÄ === INICIANDO COLETA DE NOT√çCIAS INOVAWEEK UVV ===")
        print(f"üéØ Foco: Not√≠cias do InovaWeek")
        print(f"üìÖ Per√≠odo: {self.periodo_inicio.strftime('%d/%m/%Y')} at√© {self.periodo_fim.strftime('%d/%m/%Y')}")
        print(f"üìç URL base: {self.noticias_url}")
        print(f"üìÑ M√°ximo de p√°ginas: {max_paginas if not somente_primeira_pagina else 1}")
        
        # Testar conectividade primeiro
        if not self.testar_conectividade():
            print("‚ùå Falha na conectividade. Abortando coleta.")
            return []
        
        # Coletar not√≠cias de m√∫ltiplas p√°ginas
        todas_noticias = []
        paginas_processadas = 0
        paginas_sem_noticias = 0
        
        for numero_pagina in range(1, max_paginas + 1):
            if somente_primeira_pagina and numero_pagina > 1:
                break
                
            print(f"\nüìÑ === PROCESSANDO P√ÅGINA {numero_pagina} ===")
            
            # Construir URL da p√°gina atual
            noticias_pagina = self._coletar_noticias_pagina_especifica(numero_pagina)
            
            if not noticias_pagina:
                paginas_sem_noticias += 1
                print(f"‚ö†Ô∏è P√°gina {numero_pagina}: Nenhuma not√≠cia InovaWeek encontrada")
                
                # Se n√£o encontrou not√≠cias em 3 p√°ginas consecutivas, parar
                if paginas_sem_noticias >= 3:
                    print(f"üõë Parando busca: 3 p√°ginas consecutivas sem not√≠cias InovaWeek")
                    break
                continue
            else:
                paginas_sem_noticias = 0  # Reset contador
                
            print(f"‚úÖ P√°gina {numero_pagina}: {len(noticias_pagina)} not√≠cias InovaWeek encontradas")
            todas_noticias.extend(noticias_pagina)
            paginas_processadas += 1
            
            # Rate limiting entre p√°ginas
            if numero_pagina < max_paginas:
                time.sleep(2)  # Pausa respeitosa entre p√°ginas
        
        print(f"\nüéâ === COLETA PAGINADA FINALIZADA ===")
        print(f"üìÑ P√°ginas processadas: {paginas_processadas}")
        print(f"üìä Total de not√≠cias encontradas: {len(todas_noticias)}")
        
        # Processar cada not√≠cia para extrair conte√∫do completo
        print(f"\nüìñ === EXTRAINDO CONTE√öDO COMPLETO ===")
        for i, noticia in enumerate(todas_noticias):
            if noticia.get('link') and noticia['link'] != '#':
                print(f"üìñ Processando {i+1}/{len(todas_noticias)}: {noticia['titulo'][:50]}...")
                
                conteudo_dados = self.processar_noticia_individual(noticia['link'])
                noticia.update(conteudo_dados)
                
                # Rate limiting para ser respeitoso
                time.sleep(1)
        
        self.noticias = todas_noticias
        
        print(f"\nüéâ === COLETA COMPLETA FINALIZADA ===")
        print(f"üìä Total coletado: {len(self.noticias)} not√≠cias do InovaWeek")
        print(f"üìñ Com conte√∫do completo: {sum(1 for n in self.noticias if len(n.get('conteudo_completo', '')) > 100)}")
        print(f"üìÑ P√°ginas vasculhadas: {paginas_processadas}")
        
        return self.noticias
    
    def _coletar_noticias_pagina_especifica(self, numero_pagina):
        """
        Coleta not√≠cias de uma p√°gina espec√≠fica.
        
        Args:
            numero_pagina (int): N√∫mero da p√°gina para coletar
            
        Returns:
            list: Lista de not√≠cias InovaWeek da p√°gina
        """
        # Construir URLs para diferentes formatos de pagina√ß√£o
        urls_tentar = [
            # Formato padr√£o com /page/X/
            f"{self.base_url}/noticias/page/{numero_pagina}/",
            # Formato alternativo
            f"{self.noticias_url}page/{numero_pagina}/",
            # Formato com par√¢metro
            f"{self.noticias_url}?page={numero_pagina}",
            f"{self.noticias_url}?p={numero_pagina}"
        ]
        
        # Se for p√°gina 1, tentar tamb√©m a URL base
        if numero_pagina == 1:
            urls_tentar.insert(0, self.noticias_url)
        
        for url_tentativa in urls_tentar:
            print(f"üîç Tentando URL: {url_tentativa}")
            
            response = self.fazer_requisicao(url_tentativa)
            
            if response and response.status_code == 200:
                # Verificar se a p√°gina realmente existe (n√£o √© redirect para p√°gina 1)
                if numero_pagina > 1 and 'page' not in response.url and numero_pagina != 1:
                    print(f"‚ö†Ô∏è Poss√≠vel redirect para p√°gina principal - pulando")
                    continue
                
                # Extrair not√≠cias da p√°gina
                noticias_pagina = self.extrair_noticias_pagina(response.text, url_tentativa)
                
                if noticias_pagina:
                    print(f"‚úÖ P√°gina {numero_pagina} acessada com sucesso: {len(noticias_pagina)} not√≠cias InovaWeek")
                    return noticias_pagina
                else:
                    print(f"üì≠ P√°gina {numero_pagina} acessada mas sem not√≠cias InovaWeek")
            else:
                print(f"‚ùå Falha ao acessar: {url_tentativa}")
        
        return []
    
    def verificar_paginacao_disponivel(self):
        """
        Verifica quantas p√°ginas de not√≠cias est√£o dispon√≠veis.
        
        Returns:
            int: N√∫mero estimado de p√°ginas dispon√≠veis
        """
        print(f"\nüîç === VERIFICANDO PAGINA√á√ÉO DISPON√çVEL ===")
        
        # Fazer requisi√ß√£o para p√°gina principal
        response = self.fazer_requisicao(self.noticias_url)
        if not response:
            return 1
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Procurar por indicadores de pagina√ß√£o
        links_paginacao = []
        
        # Seletores comuns para pagina√ß√£o
        seletores_paginacao = [
            'a[href*="page"]',
            '.pagination a',
            '.pager a',
            'nav a[href*="page"]',
            '.page-numbers a',
            '.wp-pagenavi a'
        ]
        
        for seletor in seletores_paginacao:
            links = soup.select(seletor)
            for link in links:
                href = link.get('href', '')
                if 'page' in href:
                    links_paginacao.append(href)
        
        # Extrair n√∫meros de p√°gina dos links
        numeros_pagina = []
        for link in links_paginacao:
            # Tentar extrair n√∫mero da p√°gina
            import re
            match = re.search(r'page[/=](\d+)', link)
            if match:
                numeros_pagina.append(int(match.group(1)))
        
        max_pagina = max(numeros_pagina) if numeros_pagina else 1
        
        print(f"üî¢ P√°ginas detectadas: {sorted(set(numeros_pagina)) if numeros_pagina else [1]}")
        print(f"üìÑ M√°xima p√°gina encontrada: {max_pagina}")
        
        # Verificar se realmente existem mais p√°ginas testando algumas
        paginas_existentes = 1
        for teste_pagina in range(2, min(max_pagina + 1, 6)):  # Testar at√© p√°gina 5
            url_teste = f"{self.base_url}/noticias/page/{teste_pagina}/"
            response_teste = self.fazer_requisicao(url_teste)
            
            if response_teste and response_teste.status_code == 200:
                # Verificar se n√£o √© redirect para p√°gina principal
                if 'page' in response_teste.url or teste_pagina == 1:
                    paginas_existentes = teste_pagina
                    print(f"‚úÖ P√°gina {teste_pagina} confirmada")
                else:
                    print(f"‚ùå P√°gina {teste_pagina} redireciona para principal")
                    break
            else:
                print(f"‚ùå P√°gina {teste_pagina} n√£o acess√≠vel")
                break
                
            time.sleep(1)  # Rate limiting
        
        print(f"üìä Total de p√°ginas confirmadas: {paginas_existentes}")
        return max(paginas_existentes, max_pagina if numeros_pagina else 1)
    
    def exportar_csv(self, filename=None, incluir_metadados=True):
        """
        Exporta as not√≠cias coletadas para arquivo CSV bem estruturado.
        
        Args:
            filename (str): Nome do arquivo (opcional)
            incluir_metadados (bool): Se deve incluir arquivo de metadados
            
        Returns:
            str: Nome do arquivo gerado
        """
        if not filename:
            timestamp = self.timestamp_scraping.strftime('%Y%m%d_%H%M%S')
            filename = f"uvv_inovaweek_noticias_{timestamp}.csv"
        
        if not self.noticias:
            print("‚ö†Ô∏è Nenhuma not√≠cia para exportar")
            return filename
        
        try:
            # Campos organizados por categoria para melhor estrutura
            fieldnames_estruturados = [
                # === IDENTIFICA√á√ÉO ===
                'id_noticia',
                'titulo',
                'slug_url',
                'url_completa',
                
                # === AUTORIA E TEMPORALIDADE ===
                'autor',
                'data_publicacao_formatada',
                'data_publicacao_iso',
                'mes_publicacao',
                'ano_publicacao',
                'timestamp_coleta_iso',
                'timestamp_coleta_formatado',
                
                # === CONTE√öDO ===
                'resumo_automatico',
                'conteudo_completo_limpo',
                'palavras_chave_encontradas',
                'tamanho_caracteres',
                'tamanho_palavras',
                'qualidade_conteudo',
                
                # === CLASSIFICA√á√ÉO ===
                'categoria_evento',
                'relevancia_inovaweek',
                'periodo_valido',
                'status_processamento',
                
                # === T√âCNICO/METADADOS ===
                'fonte_site',
                'metodo_extracao',
                'encoding_original',
                'http_status'
            ]
            
            with open(filename, 'w', newline='', encoding='utf-8') as file:
                writer = csv.DictWriter(file, fieldnames=fieldnames_estruturados)
                writer.writeheader()
                
                for i, noticia in enumerate(self.noticias, 1):
                    # Preparar dados estruturados para CSV
                    row = self._preparar_dados_estruturados(noticia, i)
                    writer.writerow(row)
            
            print(f"üíæ CSV estruturado exportado: {filename}")
            
            # Gerar arquivo de metadados se solicitado
            if incluir_metadados:
                self._gerar_arquivo_metadados(filename)
            
            # Estat√≠sticas detalhadas
            self._exibir_estatisticas_export(filename)
            
            return filename
            
        except Exception as e:
            print(f"‚ùå Erro ao exportar CSV estruturado: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _preparar_dados_estruturados(self, noticia, indice):
        """
        Prepara dados de uma not√≠cia em formato estruturado para CSV.
        
        Args:
            noticia (dict): Dados da not√≠cia
            indice (int): √çndice da not√≠cia
            
        Returns:
            dict: Dados estruturados para CSV
        """
        # Extrair informa√ß√µes do link
        url = noticia.get('link', '')
        slug = url.split('/')[-1] if url else ''
        
        # Processar conte√∫do
        conteudo = noticia.get('conteudo_completo', '')
        conteudo_limpo = self._limpar_conteudo_para_csv(conteudo)
        palavras = conteudo_limpo.split() if conteudo_limpo else []
        
        # Detectar palavras-chave do InovaWeek
        palavras_chave_encontradas = []
        conteudo_lower = conteudo_limpo.lower()
        for palavra in self.palavras_chave_inova:
            if palavra.lower() in conteudo_lower:
                palavras_chave_encontradas.append(palavra)
        
        # Determinar qualidade do conte√∫do
        qualidade = self._avaliar_qualidade_conteudo(conteudo_limpo)
        
        # Processar datas
        data_pub = noticia.get('data_publicacao')
        data_iso = data_pub.isoformat() if isinstance(data_pub, datetime) else ''
        data_formatada = data_pub.strftime('%d/%m/%Y %H:%M') if isinstance(data_pub, datetime) else ''
        mes_pub = data_pub.strftime('%Y-%m') if isinstance(data_pub, datetime) else ''
        ano_pub = data_pub.year if isinstance(data_pub, datetime) else ''
        
        # Timestamp de coleta
        timestamp_coleta = self.timestamp_scraping
        timestamp_iso = timestamp_coleta.isoformat()
        timestamp_formatado = timestamp_coleta.strftime('%d/%m/%Y %H:%M:%S')
        
        return {
            # === IDENTIFICA√á√ÉO ===
            'id_noticia': f"UVV_INOVA_{indice:03d}_{self.timestamp_scraping.strftime('%Y%m%d')}",
            'titulo': noticia.get('titulo', '').strip(),
            'slug_url': slug,
            'url_completa': url,
            
            # === AUTORIA E TEMPORALIDADE ===
            'autor': noticia.get('autor', '').strip(),
            'data_publicacao_formatada': data_formatada,
            'data_publicacao_iso': data_iso,
            'mes_publicacao': mes_pub,
            'ano_publicacao': str(ano_pub) if ano_pub else '',
            'timestamp_coleta_iso': timestamp_iso,
            'timestamp_coleta_formatado': timestamp_formatado,
            
            # === CONTE√öDO ===
            'resumo_automatico': self._gerar_resumo_automatico(conteudo_limpo),
            'conteudo_completo_limpo': conteudo_limpo,
            'palavras_chave_encontradas': ', '.join(palavras_chave_encontradas),
            'tamanho_caracteres': len(conteudo_limpo),
            'tamanho_palavras': len(palavras),
            'qualidade_conteudo': qualidade,
            
            # === CLASSIFICA√á√ÉO ===
            'categoria_evento': self._classificar_categoria_evento(noticia.get('titulo', '')),
            'relevancia_inovaweek': self._calcular_relevancia_inovaweek(noticia),
            'periodo_valido': noticia.get('periodo_valido', True),
            'status_processamento': 'COMPLETO' if conteudo_limpo else 'PARCIAL',
            
            # === T√âCNICO/METADADOS ===
            'fonte_site': 'UVV - Universidade Vila Velha',
            'metodo_extracao': 'Web Scraping - BeautifulSoup + Requests',
            'encoding_original': 'UTF-8',
            'http_status': '200'
        }
    
    def _limpar_conteudo_para_csv(self, conteudo):
        """Limpa conte√∫do para formato CSV."""
        if not conteudo:
            return ''
        
        # Remover quebras de linha duplas e m√∫ltiplas
        conteudo = re.sub(r'\n\s*\n', '\n', conteudo)
        # Remover espa√ßos extras
        conteudo = re.sub(r'\s+', ' ', conteudo)
        # Remover caracteres de controle
        conteudo = ''.join(char for char in conteudo if ord(char) >= 32 or char in '\n\t')
        
        return conteudo.strip()
    
    def _gerar_resumo_automatico(self, conteudo, max_palavras=50):
        """Gera resumo autom√°tico do conte√∫do."""
        if not conteudo:
            return ''
        
        palavras = conteudo.split()
        if len(palavras) <= max_palavras:
            return conteudo
        
        resumo = ' '.join(palavras[:max_palavras])
        return resumo + '...'
    
    def _avaliar_qualidade_conteudo(self, conteudo):
        """Avalia qualidade do conte√∫do extra√≠do."""
        if not conteudo:
            return 'VAZIO'
        
        tamanho = len(conteudo)
        if tamanho < 100:
            return 'BAIXA'
        elif tamanho < 500:
            return 'M√âDIA'
        elif tamanho < 2000:
            return 'BOA'
        else:
            return 'EXCELENTE'
    
    def _classificar_categoria_evento(self, titulo):
        """Classifica categoria do evento baseado no t√≠tulo."""
        titulo_lower = titulo.lower()
        
        if any(palavra in titulo_lower for palavra in ['palestra', 'talk', 'apresenta√ß√£o']):
            return 'PALESTRA'
        elif any(palavra in titulo_lower for palavra in ['show', 'm√∫sica', 'apresenta√ß√£o cultural']):
            return 'SHOW/CULTURAL'
        elif any(palavra in titulo_lower for palavra in ['premia√ß√£o', 'pr√™mio', 'homenagem']):
            return 'PREMIA√á√ÉO'
        elif any(palavra in titulo_lower for palavra in ['exposi√ß√£o', 'mostra', 'feira']):
            return 'EXPOSI√á√ÉO'
        elif any(palavra in titulo_lower for palavra in ['trek', 'caminhada', 'visita']):
            return 'ATIVIDADE'
        else:
            return 'GERAL'
    
    def _calcular_relevancia_inovaweek(self, noticia):
        """Calcula relev√¢ncia da not√≠cia para o InovaWeek."""
        titulo = noticia.get('titulo', '').lower()
        conteudo = noticia.get('conteudo_completo', '').lower()
        
        pontos = 0
        
        # Pontua√ß√£o por palavra-chave no t√≠tulo
        for palavra in self.palavras_chave_inova:
            if palavra.lower() in titulo:
                pontos += 3
        
        # Pontua√ß√£o por palavra-chave no conte√∫do
        for palavra in self.palavras_chave_inova:
            if palavra.lower() in conteudo:
                pontos += 1
        
        # Classificar relev√¢ncia
        if pontos >= 10:
            return 'MUITO_ALTA'
        elif pontos >= 5:
            return 'ALTA'
        elif pontos >= 2:
            return 'M√âDIA'
        else:
            return 'BAIXA'
    
    def _gerar_arquivo_metadados(self, arquivo_csv):
        """Gera arquivo de metadados complementar."""
        nome_metadados = arquivo_csv.replace('.csv', '_metadados.txt')
        
        try:
            with open(nome_metadados, 'w', encoding='utf-8') as f:
                f.write("=== METADADOS DA COLETA - UVV INOVAWEEK ===\n\n")
                f.write(f"üìÖ Data da coleta: {self.timestamp_scraping.strftime('%d/%m/%Y %H:%M:%S')}\n")
                f.write(f"üéØ Per√≠odo analisado: {self.periodo_inicio.strftime('%d/%m/%Y')} at√© {self.periodo_fim.strftime('%d/%m/%Y')}\n")
                f.write(f"üåê Site origem: {self.base_url}\n")
                f.write(f"üì∞ Total de not√≠cias: {len(self.noticias)}\n\n")
                
                f.write("=== ESTRUTURA DO CSV ===\n")
                f.write("O arquivo CSV cont√©m as seguintes se√ß√µes organizadas:\n\n")
                f.write("1. IDENTIFICA√á√ÉO: ID √∫nico, t√≠tulo, URL\n")
                f.write("2. AUTORIA E TEMPORALIDADE: Autor, datas formatadas\n")
                f.write("3. CONTE√öDO: Resumo, conte√∫do completo, palavras-chave\n")
                f.write("4. CLASSIFICA√á√ÉO: Categoria, relev√¢ncia, qualidade\n")
                f.write("5. T√âCNICO/METADADOS: Informa√ß√µes de extra√ß√£o\n\n")
                
                f.write("=== PALAVRAS-CHAVE MONITORADAS ===\n")
                for palavra in self.palavras_chave_inova:
                    f.write(f"- {palavra}\n")
                
                f.write(f"\n=== CONFIGURA√á√ïES T√âCNICAS ===\n")
                f.write(f"- Encoding: UTF-8\n")
                f.write(f"- Separador CSV: v√≠rgula (,)\n")
                f.write(f"- Tratamento de aspas: escape duplo\n")
                f.write(f"- User-Agent: {self.headers.get('User-Agent', 'N/A')[:50]}...\n")
                
            print(f"üìã Arquivo de metadados gerado: {nome_metadados}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao gerar metadados: {e}")
    
    def _exibir_estatisticas_export(self, filename):
        """Exibe estat√≠sticas detalhadas do export."""
        print(f"\nüìä === ESTAT√çSTICAS DETALHADAS DO EXPORT ===")
        
        # Estat√≠sticas b√°sicas
        total = len(self.noticias)
        com_conteudo = sum(1 for n in self.noticias if len(n.get('conteudo_completo', '')) > 100)
        com_autor = sum(1 for n in self.noticias if n.get('autor'))
        com_data = sum(1 for n in self.noticias if n.get('data_publicacao'))
        
        print(f"üìà Geral:")
        print(f"   üì∞ Total de registros: {total}")
        print(f"   üìñ Com conte√∫do completo: {com_conteudo} ({com_conteudo/total*100:.1f}%)")
        print(f"   üë§ Com autor identificado: {com_autor} ({com_autor/total*100:.1f}%)")
        print(f"   üìÖ Com data de publica√ß√£o: {com_data} ({com_data/total*100:.1f}%)")
        
        # Qualidade do conte√∫do
        qualidades = {}
        for noticia in self.noticias:
            conteudo = noticia.get('conteudo_completo', '')
            qualidade = self._avaliar_qualidade_conteudo(conteudo)
            qualidades[qualidade] = qualidades.get(qualidade, 0) + 1
        
        print(f"\nüìä Qualidade do conte√∫do:")
        for qualidade, count in sorted(qualidades.items()):
            print(f"   {qualidade}: {count} not√≠cias")
        
        # Relev√¢ncia InovaWeek
        relevancias = {}
        for noticia in self.noticias:
            relevancia = self._calcular_relevancia_inovaweek(noticia)
            relevancias[relevancia] = relevancias.get(relevancia, 0) + 1
        
        print(f"\nüéØ Relev√¢ncia InovaWeek:")
        for relevancia, count in sorted(relevancias.items(), reverse=True):
            print(f"   {relevancia}: {count} not√≠cias")
        
        print(f"\nüíæ Arquivo: {filename}")
        print(f"üîß Encoding: UTF-8")
        print(f"üìã Campos estruturados: 25 colunas organizadas")
        print(f"‚úÖ Export conclu√≠do com sucesso!")
    
    def gerar_relatorio(self):
        """Gera relat√≥rio detalhado das not√≠cias coletadas."""
        print(f"\nüìä === RELAT√ìRIO DETALHADO - INOVAWEEK UVV ===")
        
        if not self.noticias:
            print("‚ùå Nenhuma not√≠cia coletada")
            return
        
        print(f"üì∞ Total de not√≠cias do InovaWeek: {len(self.noticias)}")
        print(f"üìÖ Per√≠odo analisado: {self.periodo_inicio.strftime('%d/%m/%Y')} - {self.periodo_fim.strftime('%d/%m/%Y')}")
        
        # An√°lise por qualidade do conte√∫do
        alta_qualidade = sum(1 for n in self.noticias if len(n.get('conteudo_completo', '')) > 1000)
        media_qualidade = sum(1 for n in self.noticias if 300 <= len(n.get('conteudo_completo', '')) <= 1000)
        baixa_qualidade = sum(1 for n in self.noticias if len(n.get('conteudo_completo', '')) < 300)
        
        print(f"\nüìñ Qualidade do conte√∫do extra√≠do:")
        print(f"   üü¢ Alta qualidade (>1000 chars): {alta_qualidade}")
        print(f"   üü° M√©dia qualidade (300-1000 chars): {media_qualidade}")
        print(f"   üî¥ Baixa qualidade (<300 chars): {baixa_qualidade}")
        
        # An√°lise temporal
        noticias_com_data = [n for n in self.noticias if n.get('data_publicacao')]
        if noticias_com_data:
            datas = [n['data_publicacao'] for n in noticias_com_data]
            data_mais_antiga = min(datas)
            data_mais_recente = max(datas)
            
            print(f"\nüìÖ An√°lise temporal:")
            print(f"   üìä Not√≠cias com data: {len(noticias_com_data)}/{len(self.noticias)}")
            print(f"   üìÖ Data mais antiga: {data_mais_antiga.strftime('%d/%m/%Y')}")
            print(f"   üìÖ Data mais recente: {data_mais_recente.strftime('%d/%m/%Y')}")
        
        # Amostra de t√≠tulos
        print(f"\nüìã Amostra de not√≠cias coletadas:")
        for i, noticia in enumerate(self.noticias[:5], 1):
            status_conteudo = "‚úÖ" if len(noticia.get('conteudo_completo', '')) > 300 else "‚ùå"
            status_data = "üìÖ" if noticia.get('data_publicacao') else "‚ùì"
            
            print(f"   {i}. {status_conteudo} {status_data} {noticia['titulo'][:70]}...")
            if noticia.get('autor'):
                print(f"      üë§ Autor: {noticia['autor']}")
            if noticia.get('data_publicacao'):
                print(f"      üìÖ Data: {noticia['data_publicacao'].strftime('%d/%m/%Y')}")
        
        print(f"\nüïê Relat√≥rio gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")


def main():
    """Fun√ß√£o principal com argumentos de linha de comando."""
    parser = argparse.ArgumentParser(
        description='Scraper de not√≠cias do InovaWeek UVV (Agosto-Setembro 2025)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python scraper_uvv_inovaweek_revisado.py
  python scraper_uvv_inovaweek_revisado.py --output noticias_inovaweek.csv
  python scraper_uvv_inovaweek_revisado.py --inicio 2025-08-01 --fim 2025-09-30
        """
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        help='Nome do arquivo CSV de sa√≠da'
    )
    
    parser.add_argument(
        '--inicio',
        type=str,
        help='Data de in√≠cio (formato: YYYY-MM-DD)'
    )
    
    parser.add_argument(
        '--fim',
        type=str,
        help='Data de fim (formato: YYYY-MM-DD)'
    )
    
    parser.add_argument(
        '--seletores',
        type=str,
        help='Arquivo JSON com seletores CSS customizados'
    )
    
    parser.add_argument(
        '--max-paginas',
        type=int,
        default=10,
        help='N√∫mero m√°ximo de p√°ginas para vasculhar (padr√£o: 10)'
    )
    
    parser.add_argument(
        '--apenas-primeira-pagina',
        action='store_true',
        help='Coletar apenas da primeira p√°gina (sem pagina√ß√£o)'
    )
    
    parser.add_argument(
        '--verificar-paginacao',
        action='store_true',
        help='Verificar quantas p√°ginas est√£o dispon√≠veis antes de coletar'
    )
    
    args = parser.parse_args()
    
    # Processar datas
    periodo_inicio = None
    periodo_fim = None
    
    if args.inicio:
        try:
            periodo_inicio = datetime.strptime(args.inicio, '%Y-%m-%d')
        except ValueError:
            print("‚ùå Formato de data de in√≠cio inv√°lido. Use YYYY-MM-DD")
            sys.exit(1)
    
    if args.fim:
        try:
            periodo_fim = datetime.strptime(args.fim, '%Y-%m-%d').replace(hour=23, minute=59, second=59)
        except ValueError:
            print("‚ùå Formato de data de fim inv√°lido. Use YYYY-MM-DD")
            sys.exit(1)
    
    # Carregar seletores customizados
    seletores_customizados = None
    if args.seletores:
        try:
            with open(args.seletores, 'r', encoding='utf-8') as f:
                seletores_customizados = json.load(f)
        except Exception as e:
            print(f"‚ùå Erro ao carregar seletores customizados: {e}")
            sys.exit(1)
    
    # Executar scraping
    print("üéì === SCRAPER UVV INOVAWEEK - VERS√ÉO REVISADA ===")
    print("=" * 60)
    
    try:
        scraper = UVVInovaWeekScraper(
            periodo_inicio=periodo_inicio,
            periodo_fim=periodo_fim,
            seletores_customizados=seletores_customizados
        )
        
        # Verificar pagina√ß√£o se solicitado
        if args.verificar_paginacao:
            paginas_disponiveis = scraper.verificar_paginacao_disponivel()
            print(f"\nüìä Pagina√ß√£o dispon√≠vel: {paginas_disponiveis} p√°ginas")
            
            # Ajustar max_paginas se necess√°rio
            if args.max_paginas > paginas_disponiveis:
                print(f"‚ö†Ô∏è Ajustando max-paginas de {args.max_paginas} para {paginas_disponiveis}")
                args.max_paginas = paginas_disponiveis
        
        # Coletar not√≠cias com pagina√ß√£o
        noticias = scraper.coletar_noticias_inovaweek(
            max_paginas=args.max_paginas,
            somente_primeira_pagina=args.apenas_primeira_pagina
        )
        
        if noticias:
            # Exportar CSV
            arquivo_csv = scraper.exportar_csv(args.output)
            
            # Gerar relat√≥rio
            scraper.gerar_relatorio()
            
            print(f"\n‚úÖ Scraping conclu√≠do com sucesso!")
            print(f"üìä Total coletado: {len(noticias)} not√≠cias do InovaWeek")
            print(f"üíæ Arquivo gerado: {arquivo_csv}")
        else:
            print("‚ö†Ô∏è Nenhuma not√≠cia do InovaWeek encontrada no per√≠odo especificado")
        
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è Scraping interrompido pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro durante o scraping: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()