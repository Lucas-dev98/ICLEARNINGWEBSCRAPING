"""
Scraper Profissional de Not√≠cias - Web Scraping Avan√ßado

Este script demonstra um sistema completo e profissional de web scraping
para coleta automatizada de not√≠cias, incluindo tratamento de erros robusto,
gerenciamento de dados e exporta√ß√£o para m√∫ltiplos formatos.

Funcionalidades implementadas:
- Classe orientada a objetos para scraping estruturado
- Tratamento de requisi√ß√µes HTTP com headers customizados
- Parsing inteligente com m√∫ltiplos seletores
- Detec√ß√£o autom√°tica de links e estruturas
- Remo√ß√£o de duplicatas e limpeza de dados
- Exporta√ß√£o para CSV e DataFrame pandas
- Rate limiting para requisi√ß√µes respons√°veis
- Logging e relat√≥rios de progresso

Conceitos avan√ßados abordados:
- Programa√ß√£o orientada a objetos aplicada ao web scraping
- Gerenciamento de estado e dados persistentes
- Headers HTTP para evitar bloqueios (User-Agent spoofing)
- Delay entre requisi√ß√µes (rate limiting)
- Estruturas de dados complexas (listas de dicion√°rios)
- Integra√ß√£o com pandas para an√°lise de dados
- Manipula√ß√£o de arquivos CSV com encoding UTF-8
- Tratamento de casos edge e valida√ß√µes

Casos de uso:
- Monitoramento autom√°tico de portais de not√≠cias
- Coleta de dados para an√°lise de sentimento
- Agrega√ß√£o de conte√∫do de m√∫ltiplas fontes
- Alertas autom√°ticos sobre t√≥picos espec√≠ficos
- An√°lise de trends e padr√µes de m√≠dia
"""

# === IMPORTA√á√ïES ORGANIZADAS ===
# Requisi√ß√µes HTTP e parsing
import requests                 # Biblioteca para requisi√ß√µes HTTP/HTTPS
from bs4 import BeautifulSoup  # Parser HTML/XML avan√ßado

# Manipula√ß√£o e an√°lise de dados
import pandas as pd            # Biblioteca para an√°lise de dados estruturados
import csv                     # M√≥dulo para manipula√ß√£o de arquivos CSV

# Utilit√°rios de sistema e tempo
import time                    # Controle de delay e timing
from datetime import datetime  # Manipula√ß√£o de datas e timestamps

class NoticiasScraper:
    """
    Classe principal para scraping profissional de not√≠cias
    
    Esta classe implementa um sistema completo de web scraping orientado a objetos
    com funcionalidades avan√ßadas para coleta, processamento e armazenamento
    de dados de not√≠cias de forma escal√°vel e respons√°vel.
    
    Attributes:
        base_url (str): URL base do site para scraping
        headers (dict): Cabe√ßalhos HTTP para requisi√ß√µes
        noticias (list): Lista acumuladora de not√≠cias coletadas
    
    Design Patterns utilizados:
    - Template Method: estrutura de scraping reutiliz√°vel
    - Strategy: diferentes estrat√©gias de extra√ß√£o por site
    - Builder: constru√ß√£o incremental da cole√ß√£o de not√≠cias
    """
    
    def __init__(self, base_url, headers=None):
        """
        Inicializa o scraper com configura√ß√µes b√°sicas
        
        Args:
            base_url (str): URL base do site alvo para scraping
            headers (dict, optional): Cabe√ßalhos HTTP customizados.
                                    Se None, usa User-Agent padr√£o para evitar bloqueios.
        
        Design Notes:
        - User-Agent spoofing para parecer um navegador real
        - Headers personaliz√°veis para diferentes sites
        - Estado interno para acumular resultados entre chamadas
        """
        # === CONFIGURA√á√ÉO BASE ===
        self.base_url = base_url
        
        # === HEADERS HTTP PARA EVITAR DETEC√á√ÉO ===
        self.headers = headers or {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        # User-Agent spoofing √© crucial porque:
        # - Muitos sites bloqueiam requests com User-Agent padr√£o do Python
        # - Simula um navegador real (Chrome no Windows)
        # - Reduz chance de ser detectado como bot
        # - Alguns sites servem conte√∫do diferente baseado no User-Agent
        
        # === ARMAZENAMENTO DE DADOS ===
        self.noticias = []  # Lista acumuladora de todas as not√≠cias coletadas
        # Estrutura de cada not√≠cia: dict com chaves 'titulo', 'link', 'tag', 'timestamp'
    
    def fazer_requisicao(self, url):
        """
        Executa requisi√ß√£o HTTP com tratamento robusto de erros
        
        M√©todo centralizado para todas as requisi√ß√µes HTTP do scraper,
        implementando boas pr√°ticas de tratamento de erros e logging.
        
        Args:
            url (str): URL completa ou relativa para requisi√ß√£o
        
        Returns:
            requests.Response or None: Objeto Response se sucesso, None se erro
        
        Error Handling:
        - ConnectionError: problemas de rede/conectividade
        - Timeout: requisi√ß√£o demorou mais que o limite
        - HTTPError: status codes 4xx/5xx
        - RequestException: qualquer outro erro de requisi√ß√£o
        """
        try:
            # === EXECU√á√ÉO DA REQUISI√á√ÉO ===
            response = requests.get(url, headers=self.headers)
            # requests.get() automaticamente:
            # - Gerencia conex√µes TCP/TLS
            # - Segue redirecionamentos (at√© 30 por padr√£o)
            # - Decodifica conte√∫do baseado em Content-Type
            # - Aplica os headers especificados
            
            # === VALIDA√á√ÉO DE STATUS HTTP ===
            response.raise_for_status()
            # Levanta HTTPError se status code indica erro:
            # - 4xx: Client errors (400 Bad Request, 404 Not Found, 403 Forbidden)
            # - 5xx: Server errors (500 Internal Server Error, 502 Bad Gateway)
            # - 2xx e 3xx: considerados sucessos
            
            return response
            
        except requests.RequestException as e:
            # === TRATAMENTO UNIFICADO DE ERROS ===
            print(f"‚ùå Erro na requisi√ß√£o para {url}: {e}")
            # RequestException √© a classe base para todos os erros do requests:
            # - ConnectionError: DNS, network unreachable, connection refused
            # - Timeout: ReadTimeout, ConnectTimeout
            # - HTTPError: status codes de erro (via raise_for_status)
            # - TooManyRedirects: loop de redirecionamentos
            # - InvalidURL: URL malformada
            
            # TODO: Implementar logging mais sofisticado
            # TODO: Retry mechanism com backoff exponencial
            # TODO: Diferentes estrat√©gias por tipo de erro
            
            return None
    
    def extrair_noticias_exemplo(self, html_content):
        """
        Extrai not√≠cias usando estrat√©gia de seletores m√∫ltiplos
        
        Implementa uma abordagem gen√©rica e adapt√°vel para extra√ß√£o de not√≠cias
        que funciona com a maioria dos sites de not√≠cias, usando padr√µes comuns
        de estrutura√ß√£o de conte√∫do HTML.
        
        Args:
            html_content (str): Conte√∫do HTML bruto da p√°gina
        
        Returns:
            list: Lista de dicion√°rios com dados das not√≠cias extra√≠das
                 Estrutura: {'titulo', 'link', 'tag', 'timestamp'}
        
        Strategy Pattern:
        Esta fun√ß√£o implementa uma estrat√©gia gen√©rica que pode ser
        substitu√≠da por estrat√©gias espec√≠ficas para diferentes sites.
        
        Adaptation Notes:
        Para sites espec√≠ficos, adapte:
        1. Lista de seletores CSS
        2. L√≥gica de extra√ß√£o de links
        3. Filtros de qualidade de conte√∫do
        4. Campos adicionais espec√≠ficos do site
        """
        # === PARSING DO CONTE√öDO HTML ===
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # === LISTA ACUMULADORA DE RESULTADOS ===
        manchetes = []
        
        # === ESTRAT√âGIA DE SELETORES M√öLTIPLOS ===
        # Lista de seletores CSS ordenada por prioridade/especificidade
        seletores_prioritarios = [
            'h1',           # T√≠tulos principais (maior prioridade)
            'h2',           # Subt√≠tulos importantes
            'h3',           # T√≠tulos secund√°rios
            '.titulo',      # Classes comuns para t√≠tulos
            '.manchete',    # Classes espec√≠ficas de not√≠cias
            '.headline',    # Padr√£o internacional
            '.title',       # Varia√ß√£o comum
            '.news-title'   # Padr√£o espec√≠fico de sites de not√≠cias
        ]
        
        # === EXTRA√á√ÉO ITERATIVA POR SELETOR ===
        for seletor in seletores_prioritarios:
            elementos = soup.select(seletor)
            # select() retorna lista de elementos que correspondem ao seletor CSS
            
            for elemento in elementos:
                # === EXTRA√á√ÉO E LIMPEZA DE TEXTO ===
                texto = elemento.get_text().strip()
                # get_text(): extrai todo texto interno (sem HTML)
                # strip(): remove espa√ßos/quebras de linha nas extremidades
                
                # === FILTRO DE QUALIDADE DE CONTE√öDO ===
                if len(texto) > 20:  # Filtra textos muito curtos (provavelmente n√£o s√£o not√≠cias)
                    # TODO: Implementar filtros mais sofisticados:
                    # - Detec√ß√£o de idioma
                    # - Filtro de palavras-chave spam
                    # - An√°lise de estrutura de frase
                    
                    # === DETEC√á√ÉO INTELIGENTE DE LINKS ===
                    link = None
                    
                    # Caso 1: O pr√≥prio elemento √© um link <a>
                    if elemento.name == 'a':
                        link = elemento.get('href')
                    
                    # Caso 2: Link dentro do elemento (filho)
                    else:
                        link_tag = elemento.find('a')  # Procura primeiro <a> filho
                        if not link_tag:
                            # Caso 3: Elemento est√° dentro de um link (pai)
                            link_tag = elemento.find_parent('a')  # Procura <a> pai
                        
                        if link_tag:
                            link = link_tag.get('href')
                    
                    # === NORMALIZA√á√ÉO DE LINKS ===
                    # TODO: Implementar convers√£o de links relativos para absolutos
                    # if link and not link.startswith('http'):
                    #     link = urljoin(self.base_url, link)
                    
                    # === CONSTRU√á√ÉO DO OBJETO NOT√çCIA ===
                    manchetes.append({
                        'titulo': texto,                    # Texto limpo da manchete
                        'link': link,                       # URL da not√≠cia (pode ser None)
                        'tag': elemento.name,               # Tag HTML original (h1, h2, div, etc.)
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # Momento da coleta
                    })
                    
                    # Campos adicionais que podem ser √∫teis:
                    # - 'fonte': self.base_url
                    # - 'resumo': extra√ß√£o de primeiras linhas
                    # - 'categoria': classifica√ß√£o autom√°tica
                    # - 'autor': extra√ß√£o de byline
                    # - 'data_publicacao': parsing de datas do conte√∫do
        
        # === REMO√á√ÉO DE DUPLICATAS LOCAIS ===
        # Remove duplicatas dentro desta p√°gina baseado no t√≠tulo
        titulos_vistos = set()
        manchetes_unicas = []
        
        for manchete in manchetes:
            titulo_normalizado = manchete['titulo'].lower().strip()
            if titulo_normalizado not in titulos_vistos:
                titulos_vistos.add(titulo_normalizado)
                manchetes_unicas.append(manchete)
        
        return manchetes_unicas
    
    def scrape_site(self, url):
        """
        Executa pipeline completo de scraping para um site espec√≠fico
        
        M√©todo principal que orquestra todo o processo de coleta de dados
        de uma URL, desde a requisi√ß√£o at√© a extra√ß√£o estruturada.
        
        Args:
            url (str): URL completa do site para fazer scraping
        
        Returns:
            list: Lista de not√≠cias extra√≠das ou lista vazia se erro
        
        Pipeline Implementation:
        1. Log de in√≠cio da opera√ß√£o
        2. Requisi√ß√£o HTTP com tratamento de erros
        3. Valida√ß√£o de resposta
        4. Extra√ß√£o de dados estruturados
        5. Log de resultados
        6. Retorno de dados limpos
        """
        # === LOG DE IN√çCIO DA OPERA√á√ÉO ===
        print(f"üîç Fazendo scraping de: {url}")
        # Log importante para debugging e monitoramento de progresso
        
        # === REQUISI√á√ÉO COM TRATAMENTO DE ERROS ===
        response = self.fazer_requisicao(url)
        if not response:
            # Se requisi√ß√£o falhou, retorna lista vazia
            # Permite que o scraping continue com outras URLs
            print(f"‚ö†Ô∏è  Pulando {url} devido a erro de requisi√ß√£o")
            return []
        
        # === EXTRA√á√ÉO DE DADOS ESTRUTURADOS ===
        noticias = self.extrair_noticias_exemplo(response.text)
        # response.text cont√©m o HTML completo da p√°gina
        # extrair_noticias_exemplo() processa e estrutura os dados
        
        # === LOG DE RESULTADOS ===
        print(f"‚úÖ Encontradas {len(noticias)} not√≠cias em {url}")
        
        # === ESTAT√çSTICAS ADICIONAIS (OPCIONAL) ===
        if noticias:
            com_link = sum(1 for n in noticias if n['link'])
            print(f"   üìé {com_link}/{len(noticias)} not√≠cias com links v√°lidos")
        
        return noticias
    
    def salvar_csv(self, filename='noticias.csv'):
        """
        Exporta not√≠cias coletadas para arquivo CSV com encoding seguro
        
        M√©todo de persist√™ncia que salva todos os dados coletados em formato
        estruturado CSV, adequado para an√°lise posterior e importa√ß√£o em
        outras ferramentas (Excel, Google Sheets, bancos de dados).
        
        Args:
            filename (str): Nome do arquivo CSV de destino
                          Padr√£o: 'noticias.csv'
        
        CSV Structure:
        - titulo: Manchete/t√≠tulo da not√≠cia
        - link: URL da not√≠cia (pode ser None)
        - tag: Tag HTML original do elemento
        - timestamp: Data/hora da coleta no formato ISO
        
        Encoding & Compatibility:
        - UTF-8 para suporte completo a acentos/caracteres especiais
        - newline='' para compatibilidade com Excel
        - DictWriter para mapeamento autom√°tico de campos
        """
        # === VALIDA√á√ÉO DE DADOS ===
        if not self.noticias:
            print("‚ö†Ô∏è  Nenhuma not√≠cia coletada para salvar")
            return
        
        try:
            # === ABERTURA SEGURA DO ARQUIVO ===
            with open(filename, 'w', newline='', encoding='utf-8') as file:
                # Context manager garante fechamento mesmo com erro
                # newline='': evita linhas em branco extras no CSV
                # encoding='utf-8': suporte completo a caracteres especiais
                
                # === CONFIGURA√á√ÉO DO WRITER CSV ===
                fieldnames = ['titulo', 'link', 'tag', 'timestamp']
                writer = csv.DictWriter(file, fieldnames=fieldnames)
                # DictWriter mapeia automaticamente chaves do dict para colunas
                
                # === ESCRITA DE CABE√áALHO E DADOS ===
                writer.writeheader()            # Primeira linha com nomes das colunas
                writer.writerows(self.noticias) # Todas as linhas de dados
                
            # === CONFIRMA√á√ÉO DE SUCESSO ===
            print(f"üíæ {len(self.noticias)} not√≠cias salvas em '{filename}'")
            
        except IOError as e:
            # === TRATAMENTO DE ERROS DE ARQUIVO ===
            print(f"‚ùå Erro ao salvar CSV: {e}")
            # Poss√≠veis erros: sem permiss√£o, disco cheio, caminho inv√°lido
    
    def criar_dataframe(self):
        """
        Converte dados coletados em DataFrame pandas para an√°lise avan√ßada
        
        Transforma a lista de not√≠cias em um DataFrame pandas, estrutura
        otimizada para an√°lise de dados, visualiza√ß√£o e opera√ß√µes estat√≠sticas.
        
        Returns:
            pd.DataFrame or None: DataFrame com not√≠cias ou None se sem dados
        
        DataFrame Features:
        - Indexa√ß√£o eficiente para consultas r√°pidas
        - Opera√ß√µes vetorizadas para an√°lise em massa
        - Integra√ß√£o com matplotlib/seaborn para visualiza√ß√£o
        - Exporta√ß√£o f√°cil para m√∫ltiplos formatos (Excel, JSON, SQL)
        - Opera√ß√µes de grupo, filtro e agrega√ß√£o
        
        An√°lises poss√≠veis com o DataFrame:
        - Contagem de not√≠cias por tag HTML
        - An√°lise temporal de coletas
        - Detec√ß√£o de padr√µes em t√≠tulos
        - Estat√≠sticas de disponibilidade de links
        - Agrupamento por fonte/dom√≠nio
        """
        # === VALIDA√á√ÉO DE DADOS ===
        if not self.noticias:
            print("‚ö†Ô∏è  Nenhuma not√≠cia para converter em DataFrame")
            return None
        
        try:
            # === CRIA√á√ÉO DO DATAFRAME ===
            df = pd.DataFrame(self.noticias)
            # pandas automaticamente:
            # - Detecta tipos de dados apropriados
            # - Cria √≠ndice num√©rico sequencial
            # - Otimiza armazenamento em mem√≥ria
            # - Permite opera√ß√µes vetorizadas
            
            # === OTIMIZA√á√ïES OPCIONAIS ===
            # Convers√£o de timestamp para datetime para an√°lise temporal
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Categoriza√ß√£o da coluna 'tag' para economia de mem√≥ria
            if 'tag' in df.columns:
                df['tag'] = df['tag'].astype('category')
            
            print(f"üìä DataFrame criado com {len(df)} not√≠cias e {len(df.columns)} colunas")
            return df
            
        except Exception as e:
            # === TRATAMENTO DE ERROS DE PANDAS ===
            print(f"‚ùå Erro ao criar DataFrame: {e}")
            return None
    
    def executar_scraping(self, urls, delay=1):
        """
        Executa scraping em lote com rate limiting e deduplica√ß√£o
        
        M√©todo principal para opera√ß√µes de scraping em massa, implementando
        boas pr√°ticas de web scraping respons√°vel e processamento eficiente.
        
        Args:
            urls (list): Lista de URLs para processar sequencialmente
            delay (int): Segundos de delay entre requisi√ß√µes (padr√£o: 1s)
        
        Features Implementadas:
        - Rate limiting para n√£o sobrecarregar servidores
        - Processamento sequencial com tratamento de erros individual
        - Acumula√ß√£o progressiva de resultados
        - Deduplica√ß√£o global baseada em t√≠tulo
        - Relat√≥rios de progresso e estat√≠sticas finais
        
        Ethical Web Scraping:
        - Respeita recursos do servidor com delays
        - N√£o faz requisi√ß√µes paralelas excessivas  
        - Implementa User-Agent apropriado
        - Permite configura√ß√£o de rate limit por site
        
        Data Quality:
        - Remove duplicatas cross-site
        - Normaliza dados para consist√™ncia
        - Valida qualidade de conte√∫do extra√≠do
        """
        print(f"üöÄ Iniciando scraping de {len(urls)} URLs com delay de {delay}s")
        
        # === PROCESSAMENTO SEQUENCIAL COM RATE LIMITING ===
        for i, url in enumerate(urls, 1):
            print(f"\n--- Processando {i}/{len(urls)} ---")
            
            # === SCRAPING INDIVIDUAL DA URL ===
            noticias = self.scrape_site(url)
            
            # === ACUMULA√á√ÉO PROGRESSIVA DE RESULTADOS ===
            self.noticias.extend(noticias)
            # extend() adiciona todos os elementos da lista ao inv√©s de 
            # append() que adicionaria a lista como um √∫nico elemento
            
            print(f"üìà Total acumulado at√© agora: {len(self.noticias)} not√≠cias")
            
            # === RATE LIMITING RESPONS√ÅVEL ===
            if i < len(urls):  # N√£o faz delay ap√≥s a √∫ltima URL
                print(f"‚è≥ Aguardando {delay}s antes da pr√≥xima requisi√ß√£o...")
                time.sleep(delay)
                # Rate limiting √© essencial para:
                # - Evitar sobrecarga dos servidores
                # - Reduzir chance de ser bloqueado/banido
                # - Simular comportamento humano
                # - Respeitar robots.txt e ToS dos sites
        
        # === DEDUPLICA√á√ÉO GLOBAL AVAN√áADA ===
        print(f"\nüîÑ Removendo duplicatas...")
        titulos_vistos = set()
        noticias_unicas = []
        duplicatas_removidas = 0
        
        for noticia in self.noticias:
            # === NORMALIZA√á√ÉO DE T√çTULO PARA COMPARA√á√ÉO ===
            titulo_normalizado = noticia['titulo'].lower().strip()
            # lower(): padroniza caixa para compara√ß√£o case-insensitive
            # strip(): remove espa√ßos extras que podem causar falsos negativos
            
            # TODO: Implementar normaliza√ß√£o mais avan√ßada:
            # - Remo√ß√£o de pontua√ß√£o
            # - Stemming/lemmatiza√ß√£o para ra√≠zes de palavras
            # - Detec√ß√£o de t√≠tulos similares (fuzzy matching)
            # - Normaliza√ß√£o de encoding (acentos, caracteres especiais)
            
            if titulo_normalizado not in titulos_vistos:
                titulos_vistos.add(titulo_normalizado)
                noticias_unicas.append(noticia)
            else:
                duplicatas_removidas += 1
        
        # === ATUALIZA√á√ÉO DOS DADOS LIMPOS ===
        self.noticias = noticias_unicas
        
        # === RELAT√ìRIO FINAL DETALHADO ===
        print(f"\nüéØ === RELAT√ìRIO FINAL ===")
        print(f"‚úÖ URLs processadas: {len(urls)}")
        print(f"üì∞ Not√≠cias coletadas: {len(self.noticias)} √∫nicas")
        print(f"üóëÔ∏è  Duplicatas removidas: {duplicatas_removidas}")
        
        # === ESTAT√çSTICAS ADICIONAIS ===
        if self.noticias:
            com_links = sum(1 for n in self.noticias if n['link'])
            taxa_links = (com_links / len(self.noticias)) * 100
            print(f"üîó Not√≠cias com links: {com_links} ({taxa_links:.1f}%)")
            
            # An√°lise de tags mais comuns
            from collections import Counter
            tags_counter = Counter(n['tag'] for n in self.noticias)
            tag_mais_comum = tags_counter.most_common(1)[0] if tags_counter else ('N/A', 0)
            print(f"üè∑Ô∏è  Tag mais comum: {tag_mais_comum[0]} ({tag_mais_comum[1]} ocorr√™ncias)")
        
        print(f"‚è±Ô∏è  Tempo total estimado: {len(urls) * delay:.1f}s (delays) + tempo de processamento")

# === SE√á√ÉO DE DEMONSTRA√á√ÉO E TESTES ===

def exemplo_uso():
    """
    Fun√ß√£o de demonstra√ß√£o completa do sistema de scraping
    
    Esta fun√ß√£o exemplifica todos os recursos principais do NoticiasScraper
    usando dados HTML simulados, demonstrando um fluxo completo desde
    a coleta at√© a exporta√ß√£o dos dados.
    
    Funcionalidades demonstradas:
    1. Inicializa√ß√£o do scraper com configura√ß√µes
    2. Extra√ß√£o de dados de HTML estruturado
    3. Processamento e limpeza dos resultados
    4. An√°lise com pandas DataFrame
    5. Exporta√ß√£o para arquivo CSV
    6. Relat√≥rios e estat√≠sticas
    
    Casos de teste inclu√≠dos:
    - T√≠tulos com diferentes tags HTML (h1, h2, h3, div)
    - Links diretos e aninhados
    - Conte√∫do sem links (edge case)
    - Diferentes estruturas de markup
    """
    
    print("üß™ === DEMONSTRA√á√ÉO DO SCRAPER DE NOT√çCIAS ===\n")
    
    # === DADOS DE TESTE REALISTAS ===
    html_exemplo = """
    <html>
    <head>
        <title>Portal de Not√≠cias - Exemplo</title>
    </head>
    <body>
        <div class="noticias">
            <h1><a href="/noticia1">Brasil registra crescimento econ√¥mico no √∫ltimo trimestre</a></h1>
            <h2><a href="/noticia2">Nova tecnologia promete revolucionar energia solar</a></h2>
            <h3>Pequena not√≠cia sem link relevante</h3>
            <div class="manchete">
                <a href="/noticia3">Descoberta arqueol√≥gica importante no interior de Minas Gerais</a>
            </div>
            <p class="titulo">
                <a href="/noticia4">Startup brasileira desenvolve solu√ß√£o inovadora para agricultura</a>
            </p>
            <span class="headline">Texto muito curto</span>
            <div class="news-title">
                <a href="/noticia5">Universidades p√∫blicas lideram ranking de pesquisa cient√≠fica no pa√≠s</a>
            </div>
        </div>
    </body>
    </html>
    """
    
    # Este HTML de teste simula uma estrutura t√≠pica de portal de not√≠cias:
    # - Diferentes n√≠veis de t√≠tulos (h1, h2, h3)
    # - Classes CSS comuns (.manchete, .titulo, .headline, .news-title) 
    # - Links em diferentes estruturas (diretos, aninhados)
    # - Casos edge: sem links, textos curtos
    # - Markup variado para testar flexibilidade dos seletores
    
    # === INICIALIZA√á√ÉO DO SCRAPER ===
    print("1Ô∏è‚É£ Inicializando scraper...")
    scraper = NoticiasScraper("https://exemplo.com")
    # URL base fict√≠cia para demonstra√ß√£o
    
    # === EXTRA√á√ÉO DE DADOS ===
    print("2Ô∏è‚É£ Extraindo not√≠cias do HTML de exemplo...")
    noticias = scraper.extrair_noticias_exemplo(html_exemplo)
    scraper.noticias = noticias  # Simula coleta real
    
    # === EXIBI√á√ÉO DE RESULTADOS ESTRUTURADOS ===
    print(f"\nüì∞ === NOT√çCIAS EXTRA√çDAS ({len(noticias)} encontradas) ===")
    for i, noticia in enumerate(noticias, 1):
        print(f"\n{i}. üì∞ {noticia['titulo']}")
        print(f"   üîó Link: {noticia['link'] or 'Sem link'}")
        print(f"   üè∑Ô∏è  Tag HTML: <{noticia['tag']}>")
        print(f"   ‚è∞ Coletado: {noticia['timestamp']}")
    
    # === AN√ÅLISE COM PANDAS DATAFRAME ===
    print(f"\n3Ô∏è‚É£ Criando DataFrame para an√°lise...")
    df = scraper.criar_dataframe()
    
    if df is not None:
        print(f"\nüìä === AN√ÅLISE DOS DADOS ===")
        print(f"Dimens√µes: {df.shape[0]} linhas √ó {df.shape[1]} colunas")
        
        # Mostra preview dos dados
        print(f"\nüìã Preview do DataFrame:")
        print(df[['titulo', 'tag', 'link']].to_string(index=False, max_colwidth=50))
        
        # Estat√≠sticas b√°sicas
        print(f"\nüìà Estat√≠sticas:")
        print(f"‚Ä¢ Not√≠cias com links: {df['link'].notna().sum()}/{len(df)}")
        print(f"‚Ä¢ Tags mais usadas: {df['tag'].value_counts().to_dict()}")
        
        # An√°lise de qualidade dos t√≠tulos
        df['tamanho_titulo'] = df['titulo'].str.len()
        print(f"‚Ä¢ Tamanho m√©dio dos t√≠tulos: {df['tamanho_titulo'].mean():.1f} caracteres")
        print(f"‚Ä¢ T√≠tulo mais longo: {df['tamanho_titulo'].max()} chars")
        print(f"‚Ä¢ T√≠tulo mais curto: {df['tamanho_titulo'].min()} chars")
    
    # === EXPORTA√á√ÉO PARA CSV ===
    print(f"\n4Ô∏è‚É£ Exportando dados para CSV...")
    scraper.salvar_csv('exemplo_noticias.csv')
    
    # === DEMONSTRA√á√ÉO DE SCRAPING EM LOTE ===
    print(f"\n5Ô∏è‚É£ Simulando scraping em m√∫ltiplas URLs...")
    urls_exemplo = [
        "https://exemplo1.com/noticias",
        "https://exemplo2.com/ultimas",  
        "https://exemplo3.com/manchetes"
    ]
    
    print("URLs que seriam processadas:")
    for i, url in enumerate(urls_exemplo, 1):
        print(f"   {i}. {url}")
    
    print("\nüí° Para scraping real, descomente a linha abaixo:")
    print("# scraper.executar_scraping(urls_exemplo, delay=2)")
    
    # === DICAS DE USO AVAN√áADO ===
    print(f"\nüéØ === PR√ìXIMOS PASSOS ===")
    print("Para usar em produ√ß√£o:")
    print("1. Adapte os seletores CSS para sites espec√≠ficos")
    print("2. Implemente tratamento de JavaScript (Selenium)")
    print("3. Configure proxies/rota√ß√£o de User-Agents")
    print("4. Adicione cache/banco de dados para persist√™ncia")
    print("5. Implemente classifica√ß√£o autom√°tica de not√≠cias")
    print("6. Configure alertas para t√≥picos espec√≠ficos")
    
    print(f"\n‚úÖ Demonstra√ß√£o conclu√≠da com sucesso!")

# === EXECU√á√ÉO DA DEMONSTRA√á√ÉO ===
if __name__ == "__main__":
    # Executa apenas se o script for rodado diretamente
    exemplo_uso()
else:
    # Mensagem para quando o m√≥dulo √© importado
    print("üì¶ M√≥dulo NoticiasScraper carregado. Execute exemplo_uso() para ver demonstra√ß√£o.")